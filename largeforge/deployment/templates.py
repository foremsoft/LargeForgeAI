"""Jinja2 templates for deployment artifacts."""

# Multi-stage Dockerfile template for transformers backend
DOCKERFILE_TRANSFORMERS = """# LargeForgeAI Inference Server - Transformers Backend
# Generated by LargeForgeAI

# Stage 1: Builder
FROM {{ base_image }} AS builder

WORKDIR /app

# Install build dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \\
    build-essential \\
    git \\
    && rm -rf /var/lib/apt/lists/*

# Copy requirements and install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir --user -r requirements.txt

# Stage 2: Runtime
FROM {{ base_image }}

WORKDIR /app

# Install runtime dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \\
    libgomp1 \\
    && rm -rf /var/lib/apt/lists/*

# Copy installed packages from builder
COPY --from=builder /root/.local /root/.local
ENV PATH=/root/.local/bin:$PATH

# Copy application code
COPY main.py .
COPY config.yaml .
{% if copy_model %}
COPY {{ model_path }} /app/model
{% endif %}

# Environment variables
ENV MODEL_PATH={{ model_path_env }}
ENV PORT={{ port }}
ENV HOST=0.0.0.0
ENV PYTHONUNBUFFERED=1

# Expose port
EXPOSE {{ port }}

{% if healthcheck %}
# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \\
    CMD curl -f http://localhost:{{ port }}/health || exit 1
{% endif %}

# Run the server
CMD ["python", "main.py"]
"""

# Dockerfile template for CUDA/GPU support
DOCKERFILE_CUDA = """# LargeForgeAI Inference Server - CUDA Backend
# Generated by LargeForgeAI

FROM {{ cuda_image }}

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \\
    python3 \\
    python3-pip \\
    python3-dev \\
    build-essential \\
    git \\
    curl \\
    && rm -rf /var/lib/apt/lists/*

# Create symbolic link for python
RUN ln -s /usr/bin/python3 /usr/bin/python

# Copy and install requirements
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY main.py .
COPY config.yaml .
{% if copy_model %}
COPY {{ model_path }} /app/model
{% endif %}

# Environment variables
ENV MODEL_PATH={{ model_path_env }}
ENV PORT={{ port }}
ENV HOST=0.0.0.0
ENV PYTHONUNBUFFERED=1
ENV CUDA_VISIBLE_DEVICES={{ cuda_devices }}

# Expose port
EXPOSE {{ port }}

{% if healthcheck %}
# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=120s --retries=3 \\
    CMD curl -f http://localhost:{{ port }}/health || exit 1
{% endif %}

# Run the server
CMD ["python", "main.py"]
"""

# Dockerfile template for vLLM backend
DOCKERFILE_VLLM = """# LargeForgeAI Inference Server - vLLM Backend
# Generated by LargeForgeAI

FROM vllm/vllm-openai:{{ vllm_version }}

WORKDIR /app

# Copy configuration
COPY config.yaml .
{% if copy_model %}
COPY {{ model_path }} /app/model
{% endif %}

# Environment variables
ENV MODEL_PATH={{ model_path_env }}
ENV PORT={{ port }}

# Expose port
EXPOSE {{ port }}

{% if healthcheck %}
# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=180s --retries=3 \\
    CMD curl -f http://localhost:{{ port }}/health || exit 1
{% endif %}

# Run vLLM server
CMD ["python", "-m", "vllm.entrypoints.openai.api_server", \\
     "--model", "{{ model_path_env }}", \\
     "--host", "0.0.0.0", \\
     "--port", "{{ port }}", \\
     {% if tensor_parallel > 1 %}"--tensor-parallel-size", "{{ tensor_parallel }}",{% endif %}
     {% if max_model_len %}"--max-model-len", "{{ max_model_len }}",{% endif %}
     "--trust-remote-code"]
"""

# Docker Compose template
DOCKER_COMPOSE_TEMPLATE = """# LargeForgeAI Deployment
# Generated by LargeForgeAI
#
# Usage:
#   docker compose up -d
#   docker compose logs -f
#   docker compose down

version: '3.8'

services:
  {{ service_name }}:
{% if build_context %}
    build:
      context: {{ build_context }}
      dockerfile: Dockerfile
{% else %}
    image: {{ image }}
{% endif %}
    container_name: {{ container_name }}
    restart: unless-stopped
    ports:
      - "{{ host_port }}:{{ container_port }}"
{% if environment %}
    environment:
{% for key, value in environment.items() %}
      - {{ key }}={{ value }}
{% endfor %}
{% endif %}
{% if volumes %}
    volumes:
{% for volume in volumes %}
      - {{ volume }}
{% endfor %}
{% endif %}
{% if gpu_enabled %}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
{% if gpu_count %}
              count: {{ gpu_count }}
{% elif gpu_ids %}
              device_ids: {{ gpu_ids }}
{% else %}
              count: all
{% endif %}
              capabilities: [gpu]
{% endif %}
{% if memory_limit %}
    mem_limit: {{ memory_limit }}
{% endif %}
{% if healthcheck %}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:{{ container_port }}/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: {{ healthcheck_start_period }}
{% endif %}
{% if labels %}
    labels:
{% for key, value in labels.items() %}
      {{ key }}: "{{ value }}"
{% endfor %}
{% endif %}

{% if networks %}
networks:
{% for network in networks %}
  {{ network }}:
    external: true
{% endfor %}
{% endif %}
"""

# Requirements template
REQUIREMENTS_TEMPLATE = """# LargeForgeAI Inference Server Dependencies
# Generated by LargeForgeAI

# Core dependencies
fastapi>=0.104.0
uvicorn[standard]>=0.24.0
pydantic>=2.5.0

# Model inference
torch>=2.1.0
transformers>=4.36.0
accelerate>=0.25.0

{% if backend == "vllm" %}
# vLLM (uncomment if using vLLM backend)
# vllm>=0.2.0
{% endif %}

# LoRA support
peft>=0.7.0

# Quantization
{% if quantization %}
auto-gptq>=0.5.0
autoawq>=0.1.6
{% endif %}

# Utilities
python-multipart>=0.0.6
httpx>=0.25.0
pyyaml>=6.0.1
"""

# Main application template
SERVICE_MAIN_TEMPLATE = '''"""LargeForgeAI Inference Server."""

import os
import asyncio
from contextlib import asynccontextmanager
from typing import Dict, List, Optional, Any

import torch
import uvicorn
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from transformers import AutoModelForCausalLM, AutoTokenizer

# Configuration from environment
MODEL_PATH = os.getenv("MODEL_PATH", "{{ model_path }}")
PORT = int(os.getenv("PORT", "{{ port }}"))
HOST = os.getenv("HOST", "0.0.0.0")
MAX_NEW_TOKENS = int(os.getenv("MAX_NEW_TOKENS", "512"))
TEMPERATURE = float(os.getenv("TEMPERATURE", "0.7"))

# Global model and tokenizer
model = None
tokenizer = None


class CompletionRequest(BaseModel):
    """Request for text completion."""
    prompt: str
    max_tokens: int = Field(default=MAX_NEW_TOKENS, le=2048)
    temperature: float = Field(default=TEMPERATURE, ge=0.0, le=2.0)
    top_p: float = Field(default=0.95, ge=0.0, le=1.0)
    stop: Optional[List[str]] = None


class CompletionResponse(BaseModel):
    """Response for text completion."""
    text: str
    usage: Dict[str, int]
    finish_reason: str


class ChatMessage(BaseModel):
    """Chat message."""
    role: str
    content: str


class ChatRequest(BaseModel):
    """Request for chat completion."""
    messages: List[ChatMessage]
    max_tokens: int = Field(default=MAX_NEW_TOKENS, le=2048)
    temperature: float = Field(default=TEMPERATURE, ge=0.0, le=2.0)
    top_p: float = Field(default=0.95, ge=0.0, le=1.0)
    stop: Optional[List[str]] = None


class ChatResponse(BaseModel):
    """Response for chat completion."""
    message: ChatMessage
    usage: Dict[str, int]
    finish_reason: str


def load_model():
    """Load model and tokenizer."""
    global model, tokenizer

    print(f"Loading model from {MODEL_PATH}...")

    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_PATH,
        trust_remote_code=True,
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # Determine device and dtype
    device = "cuda" if torch.cuda.is_available() else "cpu"
    dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32

    model = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH,
        torch_dtype=dtype,
        device_map="auto" if torch.cuda.is_available() else None,
        trust_remote_code=True,
    )

    if not torch.cuda.is_available():
        model = model.to(device)

    model.eval()
    print(f"Model loaded on {device}")


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan handler."""
    load_model()
    yield


app = FastAPI(
    title="LargeForgeAI Inference Server",
    description="Generated inference server for fine-tuned LLM",
    version="1.0.0",
    lifespan=lifespan,
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.get("/health")
async def health():
    """Health check endpoint."""
    return {
        "status": "healthy",
        "model_loaded": model is not None,
        "cuda_available": torch.cuda.is_available(),
    }


@app.get("/v1/models")
async def list_models():
    """List available models."""
    return {
        "data": [
            {
                "id": MODEL_PATH,
                "object": "model",
                "owned_by": "largeforge",
            }
        ]
    }


@app.post("/v1/completions", response_model=CompletionResponse)
async def create_completion(request: CompletionRequest):
    """Create a text completion."""
    try:
        inputs = tokenizer(
            request.prompt,
            return_tensors="pt",
            padding=True,
            truncation=True,
        ).to(model.device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=request.max_tokens,
                temperature=request.temperature if request.temperature > 0 else None,
                top_p=request.top_p,
                do_sample=request.temperature > 0,
                pad_token_id=tokenizer.pad_token_id,
            )

        generated_text = tokenizer.decode(
            outputs[0][inputs["input_ids"].shape[1]:],
            skip_special_tokens=True,
        )

        return CompletionResponse(
            text=generated_text,
            usage={
                "prompt_tokens": inputs["input_ids"].shape[1],
                "completion_tokens": outputs.shape[1] - inputs["input_ids"].shape[1],
                "total_tokens": outputs.shape[1],
            },
            finish_reason="stop",
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/v1/chat/completions", response_model=ChatResponse)
async def create_chat_completion(request: ChatRequest):
    """Create a chat completion."""
    try:
        # Format messages for the model
        prompt = ""
        for msg in request.messages:
            if msg.role == "system":
                prompt += f"System: {msg.content}\\n"
            elif msg.role == "user":
                prompt += f"User: {msg.content}\\n"
            elif msg.role == "assistant":
                prompt += f"Assistant: {msg.content}\\n"
        prompt += "Assistant: "

        inputs = tokenizer(
            prompt,
            return_tensors="pt",
            padding=True,
            truncation=True,
        ).to(model.device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=request.max_tokens,
                temperature=request.temperature if request.temperature > 0 else None,
                top_p=request.top_p,
                do_sample=request.temperature > 0,
                pad_token_id=tokenizer.pad_token_id,
            )

        generated_text = tokenizer.decode(
            outputs[0][inputs["input_ids"].shape[1]:],
            skip_special_tokens=True,
        )

        return ChatResponse(
            message=ChatMessage(role="assistant", content=generated_text),
            usage={
                "prompt_tokens": inputs["input_ids"].shape[1],
                "completion_tokens": outputs.shape[1] - inputs["input_ids"].shape[1],
                "total_tokens": outputs.shape[1],
            },
            finish_reason="stop",
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    uvicorn.run(
        "main:app",
        host=HOST,
        port=PORT,
        workers=1,
    )
'''

# .dockerignore template
DOCKERIGNORE_TEMPLATE = """# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
env/
venv/
.venv/
*.egg-info/
dist/
build/

# IDE
.idea/
.vscode/
*.swp
*.swo

# Git
.git/
.gitignore

# Docker
.docker/
docker-compose*.yml
Dockerfile*

# Tests
tests/
pytest.ini
.pytest_cache/

# Documentation
docs/
*.md
!README.md

# Misc
.env.example
*.log
.DS_Store
"""

# README template
README_TEMPLATE = """# {{ bundle_name }}

LargeForgeAI Inference Server deployment package.

## Quick Start

### Using Docker Compose (Recommended)

```bash
# Start the server
docker compose up -d

# View logs
docker compose logs -f

# Stop the server
docker compose down
```

### Using Docker directly

```bash
# Build the image
docker build -t {{ image_name }} .

# Run the container
docker run -d \\
    --name {{ container_name }} \\
    -p {{ port }}:{{ port }} \\
{% if gpu_enabled %}
    --gpus all \\
{% endif %}
    {{ image_name }}
```

## API Endpoints

### Health Check
```bash
curl http://localhost:{{ port }}/health
```

### Text Completion
```bash
curl -X POST http://localhost:{{ port }}/v1/completions \\
    -H "Content-Type: application/json" \\
    -d '{
        "prompt": "Hello, how are you?",
        "max_tokens": 100,
        "temperature": 0.7
    }'
```

### Chat Completion
```bash
curl -X POST http://localhost:{{ port }}/v1/chat/completions \\
    -H "Content-Type: application/json" \\
    -d '{
        "messages": [
            {"role": "user", "content": "Hello, how are you?"}
        ],
        "max_tokens": 100,
        "temperature": 0.7
    }'
```

## Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `MODEL_PATH` | `{{ model_path }}` | Path to model directory |
| `PORT` | `{{ port }}` | Server port |
| `HOST` | `0.0.0.0` | Server host |
| `MAX_NEW_TOKENS` | `512` | Default max tokens to generate |
| `TEMPERATURE` | `0.7` | Default sampling temperature |

## Configuration

Edit `config.yaml` to customize model loading and inference settings.

{% if gpu_enabled %}
## GPU Requirements

This deployment requires NVIDIA GPU with:
- CUDA {{ cuda_version }}+ support
- At least {{ min_vram_gb }}GB VRAM
- nvidia-docker2 installed

{% endif %}
## Generated By

LargeForgeAI - Low-cost LLM training and deployment stack
"""

# Config YAML template
CONFIG_YAML_TEMPLATE = """# LargeForgeAI Inference Server Configuration
# Generated by LargeForgeAI

model:
  path: "{{ model_path }}"
  device: "{{ device }}"
  dtype: "{{ dtype }}"
  trust_remote_code: true

server:
  host: "0.0.0.0"
  port: {{ port }}
  workers: 1

generation:
  max_new_tokens: 512
  temperature: 0.7
  top_p: 0.95
  top_k: 50
  repetition_penalty: 1.1

{% if quantization %}
quantization:
  enabled: true
  method: "{{ quantization }}"
  bits: {{ quant_bits }}
{% endif %}
"""

# .env.example template
ENV_EXAMPLE_TEMPLATE = """# LargeForgeAI Inference Server Environment Variables
# Copy this file to .env and customize as needed

# Model configuration
MODEL_PATH={{ model_path }}

# Server configuration
PORT={{ port }}
HOST=0.0.0.0

# Generation defaults
MAX_NEW_TOKENS=512
TEMPERATURE=0.7

# CUDA configuration (for GPU)
# CUDA_VISIBLE_DEVICES=0
"""
